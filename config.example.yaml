# Example LogGem C  # ğŸš€ Gemma 3 Family (Recommended - Google's lightweight models):
  #   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  #   â”‚ google/gemma-3-4b-it   â”‚ 4B  â”‚  8GB RAM  â”‚ âš¡âš¡âš¡ â”‚ â­â­â­   â”‚ DEFAULT â”‚
  #   â”‚ google/gemma-3-12b-it  â”‚ 12B â”‚ 16GB RAM  â”‚ âš¡âš¡   â”‚ â­â­â­â­  â”‚ BETTER  â”‚
  #   â”‚ google/gemma-3-27b-it  â”‚ 27B â”‚ 32GB RAM  â”‚ âš¡    â”‚ â­â­â­â­â­ â”‚ BEST    â”‚
  #   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  #
  # ğŸ”„ Alternative Models:
  #   - meta-llama/Llama-3.2-3B-Instruct      (3B, 8GB RAM,  Fast)
  #   - mistralai/Mistral-7B-Instruct-v0.3    (7B, 16GB RAM, Balanced)
  #   - Qwen/Qwen2.5-7B-Instruct              (7B, 16GB RAM, Multilingual)
  #
  # ğŸ’¡ Pro Tip: Start with gemma-3-4b-it, upgrade to 12B if you have 16GB RAM
  name: "google/gemma-3-4b-it" Copy this to config.yaml and adjust as needed

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# LogGem supports multiple LLM providers. Choose one and configure accordingly.

model:
  # Provider type: huggingface, openai, anthropic, ollama
  provider: "huggingface"

  # ----- HuggingFace Provider (Local Models) -----
  # Recommended for offline use and full data privacy
  # Requires: pip install transformers torch
  
  # Model name from HuggingFace Hub
  # 
  # ğŸš€ Gemma 3 Family (Recommended - Google's lightweight models):
  #   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  #   â”‚ google/gemma-3-4b-it   â”‚ 4B  â”‚  8GB RAM  â”‚ âš¡âš¡âš¡ â”‚ â­â­â­   â”‚ DEFAULT â”‚
  #   â”‚ google/gemma-3-12b-it   â”‚ 12B â”‚ 16GB RAM  â”‚ âš¡âš¡   â”‚ â­â­â­â­  â”‚ BETTER  â”‚
  #   â”‚ google/gemma-3-27b-it  â”‚ 27B â”‚ 34GB RAM  â”‚ âš¡    â”‚ â­â­â­â­â­ â”‚ BEST    â”‚
  #   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  #
  # ğŸ”„ Alternative Models:
  #   - meta-llama/Llama-3.2-3B-Instruct      (3B, 8GB RAM,  Fast)
  #   - mistralai/Mistral-7B-Instruct-v0.3    (7B, 16GB RAM, Balanced)
  #   - Qwen/Qwen2.5-7B-Instruct              (7B, 16GB RAM, Multilingual)
  #
  # ğŸ’¡ Pro Tip: Start with gemma-3-4b-it, upgrade to 9B if you have 16GB RAM
  name: "google/gemma-3-4b-it"
  
  # Device: auto (detect best), cpu, cuda (NVIDIA), mps (Apple Silicon)
  device: "auto"
  
  # Quantization: int8 (4x smaller), fp16 (2x smaller), fp32 (full)
  # int8 recommended for most users - minimal accuracy loss
  quantization: "int8"
  
  # Model cache directory
  cache_dir: "./models"
  
  # Trust remote code (security: keep false unless you trust the model)
  trust_remote_code: false
  
  # Maximum token length
  max_length: 2048

  # ----- OpenAI Provider (Cloud API) -----
  # Requires: pip install openai
  # Requires: OPENAI_API_KEY environment variable or api_key below
  # provider: "openai"
  # name: "gpt-4o-mini"  # or gpt-4o, gpt-4-turbo, gpt-3.5-turbo
  # api_key: null  # or set OPENAI_API_KEY env var
  # base_url: null  # optional: for Azure OpenAI or custom endpoint
  # organization: null  # optional: OpenAI organization ID

  # ----- Anthropic Provider (Cloud API) -----
  # Requires: pip install anthropic
  # Requires: ANTHROPIC_API_KEY environment variable or api_key below
  # provider: "anthropic"
  # name: "claude-3-haiku-20240307"  # or claude-3-5-sonnet-20241022
  # api_key: null  # or set ANTHROPIC_API_KEY env var

  # ----- Ollama Provider (Local API) -----
  # Requires: Ollama running locally (https://ollama.ai)
  # provider: "ollama"
  # name: "llama3"  # or mistral, gemma, qwen, etc.
  # base_url: "http://localhost:11434"

# ============================================================================
# Detection Settings
# ============================================================================
detection:
  # Sensitivity: 0.0 (permissive) to 1.0 (strict)
  # Lower values reduce false positives
  # Higher values catch more anomalies but may have false positives
  sensitivity: 0.75
  
  # Batch size for processing
  # Larger = faster but more memory
  batch_size: 32
  
  # Number of previous log entries to use as context
  # More context = better detection but slower
  context_window: 100
  
  # Minimum confidence to report an anomaly
  # Higher = fewer but more certain anomalies
  min_confidence: 0.6

# Alerting Configuration
alerting:
  # Enable/disable alerting
  enabled: true
  
  # Minimum severity to alert on: low, medium, high, critical
  severity_threshold: "medium"
  
  # Maximum alerts per hour (rate limiting)
  max_alerts_per_hour: 100

# Logging Configuration
logging:
  # Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: "INFO"
  
  # Log file path (null for console only)
  file: "./logs/loggem.log"
  
  # Log rotation size
  rotation: "100 MB"
  
  # Days to retain logs
  retention: 7

# Security Configuration
security:
  # Maximum log file size to process (bytes)
  max_file_size: 1073741824  # 1 GB
  
  # Maximum length of a single log line
  max_line_length: 10000
  
  # Enable audit logging
  enable_audit_log: true
  
  # Sanitize all input data
  sanitize_input: true

# General Settings
data_dir: "./loggem_data"
temp_dir: "/tmp/loggem"
max_workers: 4
