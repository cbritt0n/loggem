#!/usr/bin/env python3
"""
Quick start script for LogGem.

Sets up the environment and runs a demo analysis with different model options.
"""

import subprocess
import sys
from pathlib import Path


def print_banner():
    """Print LogGem banner."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘   ðŸ’Ž LogGem - AI-Powered Log Analysis                   â•‘
â•‘   Intelligent anomaly detection with lightweight LLMs   â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


def run_command(cmd: list[str], description: str, show_output: bool = True) -> bool:
    """Run a command and return success status."""
    print(f"\n{'='*60}")
    print(f"  {description}")
    print(f"{'='*60}")
    try:
        if show_output:
            subprocess.run(cmd, check=True)
        else:
            subprocess.run(cmd, check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed: {e}")
        return False


def main() -> int:
    """Main setup function."""
    print_banner()

    # Check Python version
    if sys.version_info < (3, 9):
        print("âŒ Python 3.9 or higher is required")
        return 1

    print(f"âœ… Python {sys.version_info.major}.{sys.version_info.minor} detected")

    # Install LogGem
    print("\nðŸ“¦ Installing LogGem with HuggingFace support...")
    print("   (Includes Gemma 3, Llama, and other local models)")
    
    if not run_command(
        [sys.executable, "-m", "pip", "install", "-e", ".[huggingface]"],
        "Installing LogGem with dependencies",
        show_output=False
    ):
        return 1

    print("\nâœ… LogGem installed successfully!")

    # Show model options
    print("\n" + "=" * 60)
    print("  ðŸ¤– Available Lightweight Models")
    print("=" * 60)
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Model            â”‚  Size  â”‚  RAM   â”‚  Speed   â”‚  Accuracy â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Gemma 3 4B-it    â”‚  4GB   â”‚  8GB   â”‚  âš¡âš¡âš¡    â”‚  â­â­â­    â”‚
â”‚  Gemma 3 12B-it    â”‚  12GB   â”‚  16GB  â”‚  âš¡âš¡      â”‚  â­â­â­â­   â”‚
â”‚  Gemma 3 27B-it   â”‚  27GB  â”‚  34GB  â”‚  âš¡       â”‚  â­â­â­â­â­  â”‚
â”‚  Llama 3.2 3B     â”‚  3GB   â”‚  8GB   â”‚  âš¡âš¡âš¡    â”‚  â­â­â­    â”‚
â”‚  Mistral 7B       â”‚  7GB   â”‚  16GB  â”‚  âš¡âš¡      â”‚  â­â­â­â­   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Default: Gemma 3 4B-it (best balance of speed & accuracy)
    """)

    # Run a demo analysis
    print("\n" + "=" * 60)
    print("  ðŸ” Running Demo Analysis")
    print("=" * 60)

    demo_file = Path("examples/sample_auth.log")
    if not demo_file.exists():
        print(f"âš ï¸  Demo file not found: {demo_file}")
        print("Skipping demo analysis")
    else:
        print(f"\nðŸ“‹ Analyzing: {demo_file}")
        print("\nâš™ï¸  Running rule-based detection (fast demo)...")
        print("   (For AI detection, run without --no-ai flag)\n")

        # Run with --no-ai for faster demo
        run_command(
            [sys.executable, "-m", "loggem.cli", "analyze", str(demo_file), "--no-ai"],
            "LogGem Analysis - Rule-Based Detection"
        )

    # Print next steps
    print("\n" + "=" * 60)
    print("  ðŸŽ‰ Setup Complete!")
    print("=" * 60)
    print("""
ðŸ“š Quick Start Commands:

  # 1ï¸âƒ£  Rule-based detection (fast, no model download)
  loggem analyze examples/sample_auth.log --no-ai

  # 2ï¸âƒ£  AI detection with Gemma 3 4B (downloads ~4GB model first time)
  loggem analyze examples/sample_auth.log

  # 3ï¸âƒ£  Use larger model for better accuracy (config.yaml)
  model:
    provider: "huggingface"
    name: "google/gemma-3-12b-it"  # 9B model

  # 4ï¸âƒ£  Analyze different formats
  loggem analyze examples/sample_nginx.log --format nginx
  loggem analyze examples/sample_json.log --format json

  # 5ï¸âƒ£  Watch logs in real-time
  loggem watch /var/log/auth.log

  # 6ï¸âƒ£  Adjust sensitivity (0.0 - 1.0)
  loggem analyze auth.log --sensitivity 0.9

ðŸ“– Documentation:
  â€¢ README.md      - Full documentation
  â€¢ QUICKSTART.md  - Quick setup guide
  â€¢ EXAMPLES.md    - More examples & model configs
  â€¢ TESTING.md     - Development guide

ðŸ”§ Model Selection:
  â€¢ Gemma 3 4B  - Fast, 8GB RAM (default)
  â€¢ Gemma 3 12B  - Balanced, 16GB RAM (better accuracy)
  â€¢ Gemma 3 27B - Best, 34GB RAM (highest accuracy)
  â€¢ Llama 3.2   - Alternative, 8GB RAM
  â€¢ Cloud APIs  - OpenAI, Anthropic (no download needed)

ðŸ’¡ Pro Tip: Start with --no-ai for instant results, then enable AI
           detection when you need deeper analysis!

ðŸ’Ž Happy log hunting with LogGem!
    """)

    return 0


if __name__ == "__main__":
    sys.exit(main())
