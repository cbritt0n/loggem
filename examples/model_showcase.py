"""
Example: Model Showcase

Demonstrates how to use different lightweight LLM models with LogGem,
including Gemma 3 4B, Gemma 3 12B, Gemma 3 27B, Llama 3.2, and Mistral.
"""

from pathlib import Path
from loggem.detector.model_manager import ModelManager
from loggem.parsers import LogParserFactory


def print_section(title: str):
    """Print a section header."""
    print(f"\n{'='*70}")
    print(f"  {title}")
    print(f"{'='*70}\n")


def showcase_gemma_2b():
    """
    Gemma 3 4B - The Default Choice
    
    Best for: Getting started, fast inference, low resource usage
    Speed: âš¡âš¡âš¡ (Fast)
    Accuracy: â­â­â­ (Good)
    RAM: 8GB
    Download: ~4GB
    """
    print_section("ğŸš€ Gemma 3 4B - Fast & Efficient (DEFAULT)")
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Gemma 3 4B-it - The Default Model                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model ID:    google/gemma-3-4b-it                                  â”‚
â”‚  Parameters:  2 billion                                             â”‚
â”‚  Download:    ~4GB (first run only)                                 â”‚
â”‚  RAM:         8GB minimum                                           â”‚
â”‚  Speed:       âš¡âš¡âš¡ (1-2 seconds per log entry)                       â”‚
â”‚  Accuracy:    â­â­â­ (Good for most use cases)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Best For:                                                          â”‚
â”‚  â€¢ First-time users                                                 â”‚
â”‚  â€¢ Standard log analysis                                            â”‚
â”‚  â€¢ Fast inference needed                                            â”‚
â”‚  â€¢ Limited hardware (8GB RAM)                                       â”‚
â”‚  â€¢ Quick prototyping                                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Configuration (config.yaml):")
    print("""
    model:
      provider: "huggingface"
      name: "google/gemma-3-4b-it"
      device: "auto"          # GPU if available, else CPU
      quantization: "int8"    # 4x smaller, minimal accuracy loss
      cache_dir: "./models"
      max_length: 2048
    """)
    
    print("ğŸ Python Code:")
    print("""
    from loggem.detector.model_manager import ModelManager
    
    manager = ModelManager(
        provider_type="huggingface",
        provider_config={
            "model_name": "google/gemma-3-4b-it",
            "device": "auto",
            "quantization": "int8",
        }
    )
    manager.load_model()
    response = manager.generate_response("Analyze this log...")
    """)


def showcase_gemma_9b():
    """
    Gemma 3 12B - The Balanced Choice
    
    Best for: Better accuracy, production deployments
    Speed: âš¡âš¡ (Medium)
    Accuracy: â­â­â­â­ (Very Good)
    RAM: 16GB
    Download: ~12GB
    """
    print_section("âš–ï¸  Gemma 3 12B - Balanced Performance")
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Gemma 3 12B-it - The Balanced Model                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model ID:    google/gemma-3-12b-it                                  â”‚
â”‚  Parameters:  9 billion                                             â”‚
â”‚  Download:    ~12GB (first run only)                                 â”‚
â”‚  RAM:         16GB minimum                                          â”‚
â”‚  Speed:       âš¡âš¡ (3-5 seconds per log entry)                        â”‚
â”‚  Accuracy:    â­â­â­â­ (Better pattern recognition)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Best For:                                                          â”‚
â”‚  â€¢ Production deployments                                           â”‚
â”‚  â€¢ Complex log patterns                                             â”‚
â”‚  â€¢ Security-critical analysis                                       â”‚
â”‚  â€¢ Better accuracy needed                                           â”‚
â”‚  â€¢ Medium hardware (16GB RAM)                                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Configuration (config.yaml):")
    print("""
    model:
      provider: "huggingface"
      name: "google/gemma-3-12b-it"
      device: "auto"
      quantization: "int8"    # Recommended for 16GB RAM
      cache_dir: "./models"
      max_length: 2048
    """)
    
    print("ğŸ’¡ Pro Tip:")
    print("   Use int8 quantization to fit in 16GB RAM with minimal accuracy loss")


def showcase_gemma_27b():
    """
    Gemma 3 27B - The Premium Choice
    
    Best for: Highest accuracy, complex analysis
    Speed: âš¡ (Slower)
    Accuracy: â­â­â­â­â­ (Excellent)
    RAM: 34GB
    Download: ~27GB
    """
    print_section("ğŸ† Gemma 3 27B - Maximum Accuracy")
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Gemma 3 27B-it - The Premium Model                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model ID:    google/gemma-3-27b-it                                 â”‚
â”‚  Parameters:  27 billion                                            â”‚
â”‚  Download:    ~27GB (first run only)                                â”‚
â”‚  RAM:         34GB minimum (64GB recommended)                       â”‚
â”‚  Speed:       âš¡ (10-15 seconds per log entry)                       â”‚
â”‚  Accuracy:    â­â­â­â­â­ (Best pattern recognition)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Best For:                                                          â”‚
â”‚  â€¢ Mission-critical systems                                         â”‚
â”‚  â€¢ Complex security analysis                                        â”‚
â”‚  â€¢ Advanced threat detection                                        â”‚
â”‚  â€¢ Highest accuracy required                                        â”‚
â”‚  â€¢ High-end hardware (34GB+ RAM)                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Configuration (config.yaml):")
    print("""
    model:
      provider: "huggingface"
      name: "google/gemma-3-27b-it"
      device: "cuda"          # GPU highly recommended
      quantization: "int8"    # Required for 34GB RAM
      cache_dir: "./models"
      max_length: 2048
    """)
    
    print("âš ï¸  Hardware Requirements:")
    print("   â€¢ GPU: NVIDIA with 24GB+ VRAM (recommended)")
    print("   â€¢ CPU: 34GB+ RAM with int8 quantization")
    print("   â€¢ Storage: 30GB+ free space")


def showcase_alternatives():
    """Show alternative models to Gemma."""
    print_section("ğŸ”„ Alternative Models")
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Llama 3.2 3B - Fast Alternative                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model ID:    meta-llama/Llama-3.2-3B-Instruct                     â”‚
â”‚  Size:        ~3GB                                                  â”‚
â”‚  RAM:         8GB                                                   â”‚
â”‚  Speed:       âš¡âš¡âš¡ (Similar to Gemma 3B)                             â”‚
â”‚  Strength:    Good for general text analysis                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Config:")
    print("""
    model:
      name: "meta-llama/Llama-3.2-3B-Instruct"
      device: "auto"
      quantization: "int8"
    """)
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Mistral 7B - Balanced Alternative                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model ID:    mistralai/Mistral-7B-Instruct-v0.3                   â”‚
â”‚  Size:        ~7GB                                                  â”‚
â”‚  RAM:         16GB                                                  â”‚
â”‚  Speed:       âš¡âš¡ (Similar to Gemma 9B)                              â”‚
â”‚  Strength:    Strong reasoning capabilities                         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Config:")
    print("""
    model:
      name: "mistralai/Mistral-7B-Instruct-v0.3"
      device: "auto"
      quantization: "int8"
    """)
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Qwen 2.5 7B - Multilingual Alternative                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model ID:    Qwen/Qwen2.5-7B-Instruct                             â”‚
â”‚  Size:        ~7GB                                                  â”‚
â”‚  RAM:         16GB                                                  â”‚
â”‚  Speed:       âš¡âš¡                                                     â”‚
â”‚  Strength:    Excellent multilingual support                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Config:")
    print("""
    model:
      name: "Qwen/Qwen2.5-7B-Instruct"
      device: "auto"
      quantization: "int8"
    """)


def showcase_cloud_apis():
    """Show cloud API alternatives."""
    print_section("â˜ï¸  Cloud API Options (No Download)")
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  OpenAI GPT-4o Mini - Fast Cloud API                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model:       gpt-4o-mini                                           â”‚
â”‚  Cost:        ~$0.15 per 1M tokens                                  â”‚
â”‚  Speed:       âš¡âš¡âš¡ (~500ms per log)                                  â”‚
â”‚  Setup:       No download, API key only                             â”‚
â”‚  Strength:    Fast, accurate, no local resources                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Config:")
    print("""
    model:
      provider: "openai"
      name: "gpt-4o-mini"
      api_key: "sk-..."  # or set OPENAI_API_KEY env var
    """)
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  Anthropic Claude 3 Haiku - Fast Cloud API                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model:       claude-3-haiku-20240307                               â”‚
â”‚  Cost:        ~$0.25 per 1M tokens                                  â”‚
â”‚  Speed:       âš¡âš¡âš¡ (~400ms per log)                                  â”‚
â”‚  Setup:       No download, API key only                             â”‚
â”‚  Strength:    Fast inference, good reasoning                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    """)
    
    print("ğŸ“ Config:")
    print("""
    model:
      provider: "anthropic"
      name: "claude-3-haiku-20240307"
      api_key: "sk-ant-..."  # or set ANTHROPIC_API_KEY env var
    """)


def showcase_comparison_table():
    """Show detailed comparison of all models."""
    print_section("ğŸ“Š Complete Model Comparison")
    
    print("""
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Model            â”‚ Downloadâ”‚  RAM   â”‚ Speed   â”‚ Accuracy â”‚ Best For    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemma 3 4B â˜…     â”‚  4GB    â”‚  8GB   â”‚ âš¡âš¡âš¡     â”‚ â­â­â­     â”‚ Getting     â”‚
â”‚                  â”‚         â”‚        â”‚         â”‚          â”‚ Started     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemma 3 12B       â”‚  12GB    â”‚  16GB  â”‚ âš¡âš¡      â”‚ â­â­â­â­    â”‚ Production  â”‚
â”‚                  â”‚         â”‚        â”‚         â”‚          â”‚ Use         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gemma 3 27B      â”‚  27GB   â”‚  34GB  â”‚ âš¡       â”‚ â­â­â­â­â­   â”‚ Max         â”‚
â”‚                  â”‚         â”‚        â”‚         â”‚          â”‚ Accuracy    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Llama 3.2 3B     â”‚  3GB    â”‚  8GB   â”‚ âš¡âš¡âš¡     â”‚ â­â­â­     â”‚ Alternative â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mistral 7B       â”‚  7GB    â”‚  16GB  â”‚ âš¡âš¡      â”‚ â­â­â­â­    â”‚ Reasoning   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Qwen 2.5 7B      â”‚  7GB    â”‚  16GB  â”‚ âš¡âš¡      â”‚ â­â­â­â­    â”‚ Multi-      â”‚
â”‚                  â”‚         â”‚        â”‚         â”‚          â”‚ lingual     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4o Mini      â”‚  None   â”‚  None  â”‚ âš¡âš¡âš¡     â”‚ â­â­â­â­â­   â”‚ Cloud API   â”‚
â”‚                  â”‚ (API)   â”‚ (API)  â”‚         â”‚          â”‚ No Setup    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Claude 3 Haiku   â”‚  None   â”‚  None  â”‚ âš¡âš¡âš¡     â”‚ â­â­â­â­    â”‚ Cloud API   â”‚
â”‚                  â”‚ (API)   â”‚ (API)  â”‚         â”‚          â”‚ Fast        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â˜… = Default model (Gemma 3 4B)
    """)
    
    print("\nğŸ“Œ Selection Guide:")
    print("""
  ğŸš€ Starting out?           â†’ Gemma 3 4B (default)
  âš¡ Need speed + accuracy?  â†’ Gemma 3 4B or Llama 3.2 3B
  ğŸ¯ Production deployment?  â†’ Gemma 3 12B or Mistral 7B
  ğŸ† Maximum accuracy?       â†’ Gemma 3 27B
  ğŸŒ Multilingual logs?      â†’ Qwen 2.5 7B
  â˜ï¸  No local resources?    â†’ GPT-4o Mini or Claude
  ğŸ’° Budget conscious?       â†’ Local models (Gemma 3B)
  ğŸ”’ Data privacy critical?  â†’ Local models only
    """)


def performance_tips():
    """Show performance optimization tips."""
    print_section("âš¡ Performance Optimization Tips")
    
    print("""
1ï¸âƒ£  Quantization (Recommended)
   â”œâ”€ int8:  4x smaller, minimal accuracy loss
   â”œâ”€ fp16:  2x smaller, better accuracy
   â””â”€ fp32:  Full precision, largest size

   model:
     quantization: "int8"  # Best balance

2ï¸âƒ£  Device Selection
   â”œâ”€ auto:  Automatically choose best device
   â”œâ”€ cuda:  Use NVIDIA GPU (fastest)
   â”œâ”€ mps:   Use Apple Silicon GPU
   â””â”€ cpu:   Use CPU (slowest)

   model:
     device: "auto"  # Recommended

3ï¸âƒ£  Batch Processing
   detection:
     batch_size: 32     # Process logs in batches
     context_window: 100 # Include previous logs

4ï¸âƒ£  Caching
   model:
     cache_dir: "./models"  # Avoid re-downloading

5ï¸âƒ£  Hardware Acceleration
   â€¢ NVIDIA GPU: Install CUDA + cuDNN
   â€¢ Apple Silicon: Automatic with PyTorch
   â€¢ AMD GPU: ROCm support (experimental)

6ï¸âƒ£  Memory Management
   â€¢ Use int8 for large models
   â€¢ Clear cache between runs
   â€¢ Process logs in smaller batches
   â€¢ Monitor RAM usage
    """)


def main():
    """Run the showcase."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘   ğŸ’ LogGem Model Showcase                                           â•‘
â•‘   Comprehensive guide to lightweight LLM models                      â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This showcase demonstrates all available models for LogGem, helping you
choose the right model for your use case.
    """)
    
    # Show each model
    showcase_gemma_2b()      # Default: Fast & efficient
    showcase_gemma_9b()      # Balanced performance
    showcase_gemma_27b()     # Maximum accuracy
    showcase_alternatives()  # Other local models
    showcase_cloud_apis()    # Cloud options
    
    # Comparison and tips
    showcase_comparison_table()
    performance_tips()
    
    print_section("âœ… Showcase Complete")
    print("""
ğŸ¯ Quick Recommendations:

  ğŸ‘‰ New to LogGem?
     Start with Gemma 3 4B - it's fast, efficient, and works great!

  ğŸ‘‰ Have 16GB+ RAM?
     Try Gemma 3 12B for better accuracy

  ğŸ‘‰ Need best possible accuracy?
     Use Gemma 3 27B (requires 34GB RAM)

  ğŸ‘‰ Want zero setup?
     Use OpenAI GPT-4o Mini or Claude 3 Haiku

  ğŸ‘‰ Privacy concerns?
     Stick with local models (Gemma, Llama, Mistral)

ğŸ“š More Information:
  â€¢ README.md      - Full documentation
  â€¢ EXAMPLES.md    - More usage examples
  â€¢ config.yaml    - Configuration file
  â€¢ QUICKSTART.md  - Quick setup guide

ğŸ’ LogGem - Intelligent log analysis with lightweight LLMs!
    """)


if __name__ == "__main__":
    main()
